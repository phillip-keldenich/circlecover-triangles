/*
 * Copyright 2022 Phillip Keldenich, Algorithms Department, TU Braunschweig
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy of this software
 * and associated documentation files (the "Software"), to deal in the Software without restriction,
 * including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,
 * subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
 * OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
 * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 */


#pragma once

#include "i64_to_interval.hpp"

namespace ivarp {
namespace impl {
    static const __m128d SWITCH_BOTH_SIGNS128 = _mm_castsi128_pd(_mm_set_epi64x(MIN_I64, MIN_I64)); // NOLINT

    inline __m128d negate_intervald(__m128d a) noexcept IVARP_FN_PURE;
    __m128d negate_intervald(__m128d a) noexcept {
        /* No need for asm - negation is always exact. */
        a = _mm_xor_pd(a, SWITCH_BOTH_SIGNS128);
        return _mm_set_pd(a[0], a[1]);
    }

    inline __m128d add_intervald(__m128d a, __m128d b) noexcept IVARP_FN_PURE;
    __m128d add_intervald(__m128d a, __m128d b) noexcept {
        /* As for int-to-double conversion, we need to defeat over-eager constant propagation (for GCC)
         * (and other rouding-mode-breaking optimizations for Clang) using inline asm.
         * This sequence should generate almost the same code as is generated by the compiler
         * if all goes right; it should also avoid repeated loads from memory if
         * there are multiple inlined interval additions after each other. */
        a = _mm_xor_pd(a, SWITCH_UPPER_SIGN128);
        b = _mm_xor_pd(b, SWITCH_UPPER_SIGN128);
        asm("vaddpd %0, %1, %0" : "+x"(a) : "x"(b));
        return _mm_xor_pd(a, SWITCH_UPPER_SIGN128);
    }

    inline double add_rd(double x, double y) noexcept IVARP_FN_PURE;
    double add_rd(double x, double y) noexcept {
        asm("vaddsd %0, %1, %0" : "+x"(x) : "x"(y));
        return x;
    }

    inline __m128d sub_intervald(__m128d a, __m128d b) noexcept IVARP_FN_PURE;
    __m128d sub_intervald(__m128d a, __m128d b) noexcept {
        /* As for int-to-double conversion, we need to defeat over-eager constant propagation (for GCC)
         * (and other rouding-mode-breaking optimizations for Clang) using inline asm.
         * This sequence should generate almost the same code as is generated by the compiler
         * if all goes right; it should also avoid repeated loads from memory if
         * there are multiple inlined interval additions after each other. */
        __m128d lhs = _mm_set_pd(b[0], a[0]);
        __m128d rhs = _mm_set_pd(a[1], b[1]);
        asm("vsubpd %1, %0, %0" : "+x"(lhs) : "x"(rhs));
        return _mm_xor_pd(lhs, SWITCH_UPPER_SIGN128);
    }
}
}
